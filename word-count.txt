Few lawyers would be foolish enough to let an AI make their arguments, but one already did, and Judge Brantley Starr is taking steps to ensure that debacle isn’t repeated in his courtroom.

The Texas federal judge has added a requirement that any attorney appearing in his court must attest that “no portion of the filing was drafted by generative artificial intelligence,” or if it was, that it was checked “by a human being.”

Last week, attorney Steven Schwartz allowed ChatGPT to “supplement” his legal research in a recent federal filing, providing him with six cases and relevant precedent — all of which were completely hallucinated by the language model. He now “greatly regrets” doing this, and while the national coverage of this gaffe probably caused any other lawyers thinking of trying it to think again, Judge Starr isn’t taking any chances.

At the federal site for Texas’s Northern District, Starr has, like other judges, the opportunity to set specific rules for his courtroom. And added recently (though it’s unclear whether this was in response to the aforementioned filing) is the “Mandatory Certification Regarding Generative Artificial Intelligence.” Eugene Volokh first reported the news.

All attorneys appearing before the Court must file on the docket a certificate attesting either that no portion of the filing was drafted by generative artificial intelligence (such as ChatGPT, Harvey.AI, or Google Bard) or that any language drafted by generative artificial intelligence was checked for accuracy, using print reporters or traditional legal databases, by a human being.

A form for lawyers to sign is appended, noting that “quotations, citations, paraphrased assertions, and legal analysis” are all covered by this proscription. As summary is one of AI’s strong suits, and finding and summarizing precedent or previous cases is something that has been advertised as potentially helpful in legal work, this may end up coming into play more often than expected.

Whoever drafted the memorandum on this matter at Judge Starr’s office has their finger on the pulse. The certification requirement includes a pretty well informed and convincing explanation of its necessity (line breaks added for readability):

These platforms are incredibly powerful and have many uses in the law: form divorces, discovery requests, suggested errors in documents, anticipated questions at oral argument. But legal briefing is not one of them. Here’s why.

These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath.

As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle. Any party believing a platform has the requisite accuracy and reliability for legal briefing may move for leave and explain why.

In other words, be prepared to justify yourself.

While this is just one judge in one court, it would not be surprising if others took up this rule as their own. While as the court says, this is a powerful and potentially helpful technology, its use must be at the very least clearly declared and checked for accuracy.

------------------------------------

The bipedal humanoids may, in fact, be coming — but the quadrupeds are already here. They’re in labs, doing inspections in power plants and refineries, playing soccer and even — much to the concern of many — becoming cops.

Boston Dynamics’ Spot is easily the most instantly recognizable of the bunch, but plenty of startups and research institutions have put their own spin on the category. Heck, even Xiaomi made one for some reason. While the purveyors of bipeds look to prove out their work, quadrupeds are getting the job done.

The team at Google’s DeepMind (which recently absorbed a large chunk of Alphabet’s beleaguered Everyday Robots team) just issued a research paper outlining a potential benchmarking system to quantify the performance of these machines. With a name like “Barkour,” one has to wonder whether the department worked backward from the title.

Google Research points to the various impressive feats accomplished by quadrupeds over the years, from hiking up mountains to running and jumping (“flipping is much easier than walking,” an MIT professor once told me), but there hasn’t really been a baseline for determining system efficacy.


Image Credits: Google

Given that these machines are inspired by animals, the research team determined that real animals would provide the best performance analog for their robotic counterparts. That meant setting up an obstacle course in the lab and having a dog run it — check out the tenacious little wiener above. The course was composed of four obstacles in a 5×5 meter area, which it notes is denser than the dog shows that inspired it.

Performance is rated on a scale of 0 to 1 — a simple binary to determine whether the robot can successfully cross the space in the 10 or so seconds it takes for a similarly sized dog to do so. Various penalties are for slow speeds and either skipping or failing obstacles on the course. Google concludes:

We believe that developing a benchmark for legged robotics is an important first step in quantifying progress toward animal-level agility. […] Our findings demonstrate that Barkour is a challenging benchmark that can be easily customized, and that our learning-based method for solving the benchmark provides a quadruped robot with a single low-level policy that can perform a variety of agile low-level skills.

The org says that Barkour has proven an effective benchmark even in the face of the inevitable unexpected event and hardware issues. The robot dog used in the trial was able to stand back up and return to the starting line on its own in the case of failure.